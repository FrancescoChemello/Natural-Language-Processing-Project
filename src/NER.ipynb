{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkt2WG4TuxMK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Gemini libraries\n",
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To manage environment variables\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "U-v7U5e7vG7R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the dataset\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "EWlTs3froxvF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "ds, info = tfds.load(\"conll2003\", split=['train', 'dev', 'test'], with_info=True)\n",
        "train_ds, val_ds, test_ds = ds\n",
        "print(info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI9NJM_1emQy",
        "outputId": "9927dff0-25b4-4f00-f30f-8436a477346f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='conll2003',\n",
            "    full_name='conll2003/conll2003/1.0.0',\n",
            "    description=\"\"\"\n",
            "    The shared task of CoNLL-2003 concerns language-independent named entity\n",
            "    recognition and concentrates on four types of named entities: persons,\n",
            "    locations, organizations and names of miscellaneous entities that do not belong\n",
            "    to the previous three groups.\n",
            "    \"\"\",\n",
            "    homepage='https://www.aclweb.org/anthology/W03-0419/',\n",
            "    data_dir='/root/tensorflow_datasets/conll2003/conll2003/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=959.94 KiB,\n",
            "    dataset_size=3.87 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'chunks': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=23)),\n",
            "        'ner': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=9)),\n",
            "        'pos': Sequence(ClassLabel(shape=(), dtype=int64, num_classes=47)),\n",
            "        'tokens': Sequence(Text(shape=(), dtype=string)),\n",
            "    }),\n",
            "    supervised_keys=None,\n",
            "    disable_shuffling=False,\n",
            "    nondeterministic_order=False,\n",
            "    splits={\n",
            "        'dev': <SplitInfo num_examples=3251, num_shards=1>,\n",
            "        'test': <SplitInfo num_examples=3454, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=14042, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,\n",
            "        title = \"Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition\",\n",
            "        author = \"Tjong Kim Sang, Erik F.  and\n",
            "          De Meulder, Fien\",\n",
            "        booktitle = \"Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003\",\n",
            "        year = \"2003\",\n",
            "        url = \"https://www.aclweb.org/anthology/W03-0419\",\n",
            "        pages = \"142--147\",\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example in train_ds.take(3):\n",
        "    print(\"Tokens: \", example[\"tokens\"].numpy())\n",
        "    print(\"NER tags:\", example[\"ner\"].numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIGAZukygHnU",
        "outputId": "a5f7d6c1-71be-430f-a94f-3e99df94c3a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  [b'\"' b'If' b'they' b\"'re\" b'saying' b'at' b'least' b'20' b'percent' b','\n",
            " b'then' b'their' b'internal' b'forecasts' b'are' b'probably' b'saying'\n",
            " b'25' b'or' b'30' b'percent' b',' b'\"' b'said' b'one' b'Sydney' b'media'\n",
            " b'analyst' b'who' b'declined' b'to' b'be' b'named' b'.']\n",
            "NER tags: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0]\n",
            "Tokens:  [b'Lauck' b\"'s\" b'lawyer' b'vowed' b'he' b'would' b'appeal' b'against'\n",
            " b'the' b'court' b\"'s\" b'decision' b',' b'arguing' b'that' b'his'\n",
            " b'client' b'should' b'have' b'been' b'set' b'free' b'because' b'he'\n",
            " b'had' b'not' b'committed' b'any' b'offence' b'under' b'German' b'law'\n",
            " b'.']\n",
            "NER tags: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0]\n",
            "Tokens:  [b'Thailand' b\"'s\" b'powerful' b'military' b'thinks' b'the' b'government'\n",
            " b'is' b'dishonest' b'and' b'Prime' b'Minister' b'Banharn' b'Silpa-archa'\n",
            " b\"'s\" b'resignation' b'might' b'solve' b'the' b'nation' b\"'s\"\n",
            " b'political' b'and' b'economic' b'woes' b',' b'an' b'opinion' b'poll'\n",
            " b'showed' b'on' b'Thursday' b'.']\n",
            "NER tags: [5 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATASET**\n",
        "\n",
        "*NER tags*:\n",
        "\n",
        "- **ORGANIZATION** such as *Georgia-Pacific Corp.*, *WHO\n",
        "- **PERSON** such as *Eddy Bonte*, *President Obama*.\n",
        "- **LOCATION** such as *Murray River*, *Mount Everest*.\n",
        "- **MISCELLANEOUS** - Miscellaneous entities such as *events*, *nationalities*, *products*, or *works of art*."
      ],
      "metadata": {
        "id": "o9t4jGplwXdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_label_names = info.features[\"ner\"].names\n",
        "print(ner_label_names)  # ['O', 'B-PER', 'I-PER', 'B-ORG', ...]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVYUC9nQgrfy",
        "outputId": "2fc89e2b-faeb-442f-8ae8-c72f540dc17e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "# Iterate through the dataset correctly\n",
        "for example in train_ds.take(3):\n",
        "    # Access the tokens feature within each example\n",
        "    tokens = example[\"tokens\"].numpy().astype(str)\n",
        "    sentence = \" \".join(tokens)\n",
        "    sentences.append(sentence)"
      ],
      "metadata": {
        "id": "xEB689swhFA9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in sentences:\n",
        "  print(\"Example of a phrase: \", s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH3NRd_Liqqx",
        "outputId": "f1c84646-92a6-4523-8535-71e70eed49c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of a phrase:  \" If they 're saying at least 20 percent , then their internal forecasts are probably saying 25 or 30 percent , \" said one Sydney media analyst who declined to be named .\n",
            "Example of a phrase:  Lauck 's lawyer vowed he would appeal against the court 's decision , arguing that his client should have been set free because he had not committed any offence under German law .\n",
            "Example of a phrase:  Thailand 's powerful military thinks the government is dishonest and Prime Minister Banharn Silpa-archa 's resignation might solve the nation 's political and economic woes , an opinion poll showed on Thursday .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "JOLMbAyYpn3_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter for the minumum number of NER tags that an example should have\n",
        "# to be taken into consideration.\n",
        "min_num_tags = 4"
      ],
      "metadata": {
        "id": "qeMm8fFMrgi2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random shuffle of the examples in the training set\n",
        "\n",
        "dataset_array = []\n",
        "\n",
        "for example in train_ds:\n",
        "  # Count the number of NER tags in the example\n",
        "  num_tags = 0\n",
        "  for tag in example[\"ner\"].numpy():\n",
        "    if tag != 0:\n",
        "      num_tags += 1\n",
        "  if num_tags >= min_num_tags:\n",
        "    # Build the example\n",
        "    tokens = example[\"tokens\"].numpy().astype(str)\n",
        "    sentence = \" \".join(tokens)\n",
        "    # Build the NER tag array\n",
        "    ner_tags = example[\"ner\"].numpy()\n",
        "    ner_tag_array = []\n",
        "    for tag in ner_tags:\n",
        "      ner_tag_array.append(ner_label_names[tag])\n",
        "    dataset_array.append((sentence, ner_tag_array))\n",
        "\n",
        "# Print an example\n",
        "print(\"Example: \", dataset_array[0])\n",
        "\n",
        "random.seed(77)\n",
        "\n",
        "# Shuffle with the random seed\n",
        "random.shuffle(dataset_array)\n",
        "\n",
        "# Keep only the first 60 examples\n",
        "dataset_array = dataset_array[:60]\n",
        "\n",
        "dataset_sentences_array = []\n",
        "dataset_labels_array = []\n",
        "\n",
        "# Create the two arrays for labels and examples\n",
        "for (x, y) in dataset_array:\n",
        "  dataset_sentences_array.append(x)\n",
        "  dataset_labels_array.append(y)\n",
        "\n",
        "# Print the length\n",
        "print(\"Len sentence array: \", len(dataset_sentences_array))\n",
        "print(\"Len true label array: \", len(dataset_labels_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUlz15tPm4ql",
        "outputId": "8185a271-e9e5-41b2-cb66-8070646fa868"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example:  (\"A forensic scientist who examined the supposed skull of 19th century King Hintsa , a chief of President Nelson Mandela 's Xhosa tribe killed in battle by the British , said it was in fact the cranium of a European woman .\", ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O'])\n",
            "Len sentence array:  60\n",
            "Len true label array:  60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the words releated to a NER tag\n",
        "\n",
        "print(\"A sentence before the operation: \", dataset_sentences_array[1])\n",
        "print(\"A true label before the operation: \", dataset_labels_array[1])\n",
        "\n",
        "wordpair_sentence_label = []\n",
        "\n",
        "for i in range(0, len(dataset_labels_array)):\n",
        "  wordpair_sentence = []\n",
        "  sentence = dataset_sentences_array[i]\n",
        "  label = dataset_labels_array[i]\n",
        "  for i in range (0, len(label)):\n",
        "    if label[i] != 'O':\n",
        "      wordpair_sentence.append(((sentence.split(\" \"))[i], label[i]))\n",
        "  wordpair_sentence_label.append(wordpair_sentence)\n",
        "\n",
        "print(\"A sentence after the operation: \", wordpair_sentence_label[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDdfu-Jvu7WU",
        "outputId": "cfe8c41c-f4f6-4319-9b72-1af404613124"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A sentence before the operation:  144 Peter O'Malley ( Australia ) 71 73 , Costantino Rocca ( Italy )\n",
            "A true label before the operation:  ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O']\n",
            "A sentence after the operation:  [('Peter', 'B-PER'), (\"O'Malley\", 'I-PER'), ('Australia', 'B-LOC'), ('Costantino', 'B-PER'), ('Rocca', 'I-PER'), ('Italy', 'B-LOC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini** for NER *(Name Entity Recognition)* task"
      ],
      "metadata": {
        "id": "Ok-bxoU0wZps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import for regular expression\n",
        "import re\n",
        "\n",
        "# Analysis of the performance\n",
        "\"\"\"\n",
        "This function calculate the value of:\n",
        "- true positive (tp).\n",
        "- false positive (fp).\n",
        "- false negative (fn).\n",
        "\n",
        "Returns: tp, fp, fn\n",
        "\"\"\"\n",
        "def metrics (predicted, true):\n",
        "\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  # Pattern that finds all the tuples inside the response\n",
        "  predicted_tuple = re.findall(r'[\"\\'](.*?)[\"\\']\\s*,\\s*[\"\\'](.*?)[\"\\']', predicted)\n",
        "\n",
        "  # Check if there are composed tuples and scompose them if present -> ex. [\"New Year\", \"B-eve\"] to [\"New\", \"B-eve\"], [\"Year\", \"I-eve\"]\n",
        "  for (x,y) in predicted_tuple:\n",
        "    if len(x.split(\" \")) > 1:\n",
        "      predicted_tuple.remove((x,y))\n",
        "      # Get the tag (last part)\n",
        "      y = y.split(\"-\")[1]\n",
        "      first_word = True\n",
        "      for word in x.split(\" \"):\n",
        "        # First B-y, other I-y\n",
        "        if first_word:\n",
        "          first_word = False\n",
        "          predicted_tuple.append((word, \"B-\" + y))\n",
        "        else:\n",
        "          predicted_tuple.append((word, \"I-\" + y))\n",
        "\n",
        "  print(\"Predicted vector after normalization: \", predicted_tuple)\n",
        "\n",
        "  # Calculate the true positive (tp) and the false positive (fp)\n",
        "  temp_true = true.copy()\n",
        "  for tup in predicted_tuple:\n",
        "    if tup in temp_true:\n",
        "      tp += 1\n",
        "      temp_true.remove(tup)\n",
        "    else:\n",
        "      fp += 1\n",
        "\n",
        "  # Calculate the false negative (fn)\n",
        "  for tup in true:\n",
        "    if tup not in predicted_tuple:\n",
        "      fn += 1\n",
        "\n",
        "  return tp, fp, fn"
      ],
      "metadata": {
        "id": "hNDJoq16s941"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the API key from the environment of Google Colab (aka Secrets)\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "O0d6df-Au9Fe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp, fp, fn = 0, 0, 0\n",
        "\n",
        "# for i in range (0, len(dataset_sentences_array)):\n",
        "for i in range (0, 30):\n",
        "\n",
        "  # Prompt for the NER task\n",
        "  prompt = \"\"\"Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
        "  The only entity labels that could appear in the phrase are:\n",
        "    - Location (LOC).\n",
        "    - Organization (ORG).\n",
        "    - Person (PER).\n",
        "    - Miscellaneos (MISC).\n",
        "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
        "\n",
        "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
        "\n",
        "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
        "\n",
        "  Answer with less words as possible.\n",
        "\n",
        "  Your phrase is: \"\"\" + dataset_sentences_array[i] + \"\"\"\n",
        "\n",
        "  Result:\n",
        "  \"\"\"\n",
        "\n",
        "  print(prompt)\n",
        "\n",
        "  try:\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=prompt,\n",
        "    )\n",
        "\n",
        "    # response = response.text\n",
        "    print(response.text)\n",
        "\n",
        "\n",
        "    tem_tp, tem_fp, tem_fn = metrics(response.text, wordpair_sentence_label[i])\n",
        "\n",
        "    print(\"\\nTure vector [\" + str(wordpair_sentence_label[i]) + \"]\")\n",
        "    print(\"tp: \", tem_tp)\n",
        "    print(\"fp: \", tem_fp)\n",
        "    print(\"fn: \", tem_fn)\n",
        "\n",
        "    tp += tem_tp\n",
        "    fp += tem_fp\n",
        "    fn += tem_fn\n",
        "\n",
        "    # Sleep to not exeed the API limit\n",
        "    time.sleep(6)\n",
        "\n",
        "  # An error occured (probably Gemini is overloaded), so wait 15 seconds\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred for sentence {i}: {e}\")\n",
        "    time.sleep(15)\n",
        "\n",
        "    # Try again\n",
        "    i = i - 1"
      ],
      "metadata": {
        "id": "rFCnMwpgv3XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b1c8e0-bde2-4cb9-b015-7243a80b8ff9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Scorers : Ewald Brenner ( 5th minute ) , Mario Stieglmair ( 42nd ) , Ronald Brunmayr ( 43rd and 56th ) .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Ewald\", \"B-PER\"), (\"Brenner\", \"I-PER\"), (\"Mario\", \"B-PER\"), (\"Stieglmair\", \"I-PER\"), (\"Ronald\", \"B-PER\"), (\"Brunmayr\", \"I-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Ewald', 'B-PER'), ('Brenner', 'I-PER'), ('Mario', 'B-PER'), ('Stieglmair', 'I-PER'), ('Ronald', 'B-PER'), ('Brunmayr', 'I-PER')]\n",
            "\n",
            "Ture vector [[('Ewald', 'B-PER'), ('Brenner', 'I-PER'), ('Mario', 'B-PER'), ('Stieglmair', 'I-PER'), ('Ronald', 'B-PER'), ('Brunmayr', 'I-PER')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 144 Peter O'Malley ( Australia ) 71 73 , Costantino Rocca ( Italy )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Peter\", \"B-PER\"), (\"O'Malley\", \"I-PER\"), (\"Australia\", \"B-LOC\"), (\"Costantino\", \"B-PER\"), (\"Rocca\", \"I-PER\"), (\"Italy\", \"B-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Peter', 'B-PER'), (\"O'Malley\", 'I-PER'), ('Australia', 'B-LOC'), ('Costantino', 'B-PER'), ('Rocca', 'I-PER'), ('Italy', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Peter', 'B-PER'), (\"O'Malley\", 'I-PER'), ('Australia', 'B-LOC'), ('Costantino', 'B-PER'), ('Rocca', 'I-PER'), ('Italy', 'B-LOC')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Leading rider Jason Weaver received a 21-day ban from the disciplinary committee of the Jockey Club on Wednesday .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Jason\", \"B-PER\"), (\"Weaver\", \"I-PER\"), (\"Jockey\", \"B-ORG\"), (\"Club\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Jason', 'B-PER'), ('Weaver', 'I-PER'), ('Jockey', 'B-ORG'), ('Club', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Jason', 'B-PER'), ('Weaver', 'I-PER'), ('Jockey', 'B-ORG'), ('Club', 'I-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: ( N. Fairbrother 86 , M. Watkinson 64 ; D. Gough 4-53 ) and 210-5\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"N. Fairbrother\", \"B-PER\"), (\"M. Watkinson\", \"B-PER\"), (\"D. Gough\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('M. Watkinson', 'B-PER'), ('N.', 'B-PER'), ('Fairbrother', 'I-PER'), ('D.', 'B-PER'), ('Gough', 'I-PER')]\n",
            "\n",
            "Ture vector [[('N.', 'B-PER'), ('Fairbrother', 'I-PER'), ('M.', 'B-PER'), ('Watkinson', 'I-PER'), ('D.', 'B-PER'), ('Gough', 'I-PER')]]\n",
            "tp:  4\n",
            "fp:  1\n",
            "fn:  2\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: COMEX copper ended higher after a late , modest recovery dragged the market from the lows , but traders shrugged-off an imminent strike at Codelco 's Salvador mine in Chile .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"COMEX\", \"B-ORG\"), (\"Codelco\", \"B-ORG\"), (\"Salvador\", \"B-LOC\"), (\"Chile\", \"B-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('COMEX', 'B-ORG'), ('Codelco', 'B-ORG'), ('Salvador', 'B-LOC'), ('Chile', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('COMEX', 'B-ORG'), ('Codelco', 'B-ORG'), ('Salvador', 'B-LOC'), ('Chile', 'B-LOC')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 220 Barry Lane 73 77 70 , Wayne Riley ( Australia ) 71 78 71 ,\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```\n",
            "[(\"Wayne\", \"B-PER\"), (\"Riley\", \"I-PER\"), (\"Australia\", \"B-LOC\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Wayne', 'B-PER'), ('Riley', 'I-PER'), ('Australia', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Barry', 'B-PER'), ('Lane', 'I-PER'), ('Wayne', 'B-PER'), ('Riley', 'I-PER'), ('Australia', 'B-LOC')]]\n",
            "tp:  3\n",
            "fp:  0\n",
            "fn:  2\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: RUGBY LEAGUE - EUROPEAN SUPER LEAGUE RESULTS / STANDINGS .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"EUROPEAN\", \"B-MISC\"), (\"SUPER\", \"I-MISC\"), (\"LEAGUE\", \"I-MISC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('EUROPEAN', 'B-MISC'), ('SUPER', 'I-MISC'), ('LEAGUE', 'I-MISC')]\n",
            "\n",
            "Ture vector [[('RUGBY', 'B-MISC'), ('LEAGUE', 'I-MISC'), ('EUROPEAN', 'B-MISC'), ('SUPER', 'I-MISC'), ('LEAGUE', 'I-MISC')]]\n",
            "tp:  3\n",
            "fp:  0\n",
            "fn:  1\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 1. Takuma Aoki ( Japan ) Honda 38:18.759\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Takuma\", \"B-PER\"), (\"Aoki\", \"I-PER\"), (\"Japan\", \"B-LOC\"), (\"Honda\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Takuma', 'B-PER'), ('Aoki', 'I-PER'), ('Japan', 'B-LOC'), ('Honda', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Takuma', 'B-PER'), ('Aoki', 'I-PER'), ('Japan', 'B-LOC'), ('Honda', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Gloria Bistrita - Ilie Lazar ( 32nd ) , Eugen Voica ( 84th )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Gloria\", \"B-ORG\"), (\"Bistrita\", \"I-ORG\"), (\"Ilie\", \"B-PER\"), (\"Lazar\", \"I-PER\"), (\"Eugen\", \"B-PER\"), (\"Voica\", \"I-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Gloria', 'B-ORG'), ('Bistrita', 'I-ORG'), ('Ilie', 'B-PER'), ('Lazar', 'I-PER'), ('Eugen', 'B-PER'), ('Voica', 'I-PER')]\n",
            "\n",
            "Ture vector [[('Gloria', 'B-ORG'), ('Bistrita', 'I-ORG'), ('Ilie', 'B-PER'), ('Lazar', 'I-PER'), ('Eugen', 'B-PER'), ('Voica', 'I-PER')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: The ECOMOG force , currently 10,000 strong , was sent to Liberia by the Economic Community of West African States in 1990 at the height of the fighting .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"ECOMOG\", \"B-ORG\"), (\"Liberia\", \"B-LOC\"), (\"Economic Community of West African States\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('ECOMOG', 'B-ORG'), ('Liberia', 'B-LOC'), ('Economic', 'B-ORG'), ('Community', 'I-ORG'), ('of', 'I-ORG'), ('West', 'I-ORG'), ('African', 'I-ORG'), ('States', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('ECOMOG', 'B-ORG'), ('Liberia', 'B-LOC'), ('Economic', 'B-ORG'), ('Community', 'I-ORG'), ('of', 'I-ORG'), ('West', 'I-ORG'), ('African', 'I-ORG'), ('States', 'I-ORG')]]\n",
            "tp:  8\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Phan 's accidental journey started last week in Prince Rupert , British Columbia , where he was searching for a fishing job , Jewell said .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Phan\", \"B-PER\"), (\"Prince Rupert\", \"B-LOC\"), (\"British Columbia\", \"B-LOC\"), (\"Jewell\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Phan', 'B-PER'), ('British Columbia', 'B-LOC'), ('Jewell', 'B-PER'), ('Prince', 'B-LOC'), ('Rupert', 'I-LOC')]\n",
            "\n",
            "Ture vector [[('Phan', 'B-PER'), ('Prince', 'B-LOC'), ('Rupert', 'I-LOC'), ('British', 'B-LOC'), ('Columbia', 'I-LOC'), ('Jewell', 'B-PER')]]\n",
            "tp:  4\n",
            "fp:  1\n",
            "fn:  2\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: An earthquake measuring 5.5 on the Richter scale shook New Zealands upper South Island on Thursday but there were no reports of injuries , Television New Zealand said .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"New\", \"B-LOC\"), (\"Zealand\", \"I-LOC\"), (\"South\", \"B-LOC\"), (\"Island\", \"I-LOC\"), (\"Television\", \"B-ORG\"), (\"New\", \"I-ORG\"), (\"Zealand\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('New', 'B-LOC'), ('Zealand', 'I-LOC'), ('South', 'B-LOC'), ('Island', 'I-LOC'), ('Television', 'B-ORG'), ('New', 'I-ORG'), ('Zealand', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Richter', 'B-PER'), ('New', 'B-MISC'), ('Zealands', 'I-MISC'), ('South', 'B-LOC'), ('Island', 'I-LOC'), ('Television', 'B-ORG'), ('New', 'I-ORG'), ('Zealand', 'I-ORG')]]\n",
            "tp:  5\n",
            "fp:  2\n",
            "fn:  3\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Daewoo Dacom Communications ( Poland ) Ltd , which was set up with an initial investment of $ 1.0 million , is expected to have sales of $ 60 million by the year 2000 , a Daewoo statement said on Thursday ..\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Daewoo\", \"B-ORG\"), (\"Dacom\", \"I-ORG\"), (\"Communications\", \"I-ORG\"), (\"Poland\", \"B-LOC\"), (\"Ltd\", \"I-ORG\"), (\"Daewoo\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Daewoo', 'B-ORG'), ('Dacom', 'I-ORG'), ('Communications', 'I-ORG'), ('Poland', 'B-LOC'), ('Ltd', 'I-ORG'), ('Daewoo', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Daewoo', 'B-ORG'), ('Dacom', 'I-ORG'), ('Communications', 'I-ORG'), ('Poland', 'B-LOC'), ('Ltd', 'B-ORG'), ('Daewoo', 'B-ORG')]]\n",
            "tp:  5\n",
            "fp:  1\n",
            "fn:  1\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Christie will run the anchor leg after Canada 's Bailey , American Johnson and Olympic silver medallist Fredericks have run the first three stages of the relay .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Christie\", \"B-PER\"), (\"Canada\", \"B-LOC\"), (\"Bailey\", \"B-PER\"), (\"American\", \"B-MISC\"), (\"Johnson\", \"B-PER\"), (\"Olympic\", \"B-MISC\"), (\"Fredericks\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Christie', 'B-PER'), ('Canada', 'B-LOC'), ('Bailey', 'B-PER'), ('American', 'B-MISC'), ('Johnson', 'B-PER'), ('Olympic', 'B-MISC'), ('Fredericks', 'B-PER')]\n",
            "\n",
            "Ture vector [[('Christie', 'B-PER'), ('Canada', 'B-LOC'), ('Bailey', 'B-PER'), ('American', 'B-MISC'), ('Johnson', 'B-PER'), ('Olympic', 'B-MISC'), ('Fredericks', 'B-PER')]]\n",
            "tp:  7\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Groningen took the lead in the seventh minute when Dean Gorre intercepted a back pass from Ernest Faber to goalkeeper Ronald Waterreus and shot home .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Groningen\", \"B-LOC\"), (\"Dean Gorre\", \"B-PER\"), (\"Ernest Faber\", \"B-PER\"), (\"Ronald Waterreus\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Groningen', 'B-LOC'), ('Ernest Faber', 'B-PER'), ('Dean', 'B-PER'), ('Gorre', 'I-PER'), ('Ronald', 'B-PER'), ('Waterreus', 'I-PER')]\n",
            "\n",
            "Ture vector [[('Groningen', 'B-ORG'), ('Dean', 'B-PER'), ('Gorre', 'I-PER'), ('Ernest', 'B-PER'), ('Faber', 'I-PER'), ('Ronald', 'B-PER'), ('Waterreus', 'I-PER')]]\n",
            "tp:  4\n",
            "fp:  2\n",
            "fn:  3\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: The church in Australia said on Monday Lynch , Batchelor , Barton and Riel were held in a prison until the weekend , when they were moved to join the other captives at the compound .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Australia\", \"B-LOC\"), (\"Lynch\", \"B-PER\"), (\"Batchelor\", \"B-PER\"), (\"Barton\", \"B-PER\"), (\"Riel\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Australia', 'B-LOC'), ('Lynch', 'B-PER'), ('Batchelor', 'B-PER'), ('Barton', 'B-PER'), ('Riel', 'B-PER')]\n",
            "\n",
            "Ture vector [[('Australia', 'B-LOC'), ('Lynch', 'B-PER'), ('Batchelor', 'B-PER'), ('Barton', 'B-PER'), ('Riel', 'B-LOC')]]\n",
            "tp:  4\n",
            "fp:  1\n",
            "fn:  1\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Manchester United 2 ( Cruyff 39th minute , Solskjaer 70th )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Manchester\", \"B-ORG\"), (\"United\", \"I-ORG\"), (\"Cruyff\", \"B-PER\"), (\"Solskjaer\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Manchester', 'B-ORG'), ('United', 'I-ORG'), ('Cruyff', 'B-PER'), ('Solskjaer', 'B-PER')]\n",
            "\n",
            "Ture vector [[('Manchester', 'B-ORG'), ('United', 'I-ORG'), ('Cruyff', 'B-PER'), ('Solskjaer', 'B-PER')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: England beat Pakistan by five wickets to win the first one-day ( 50 overs-a-side ) international at Old Trafford on Thursday .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"England\", \"B-LOC\"), (\"Pakistan\", \"B-LOC\"), (\"Old\", \"B-LOC\"), (\"Trafford\", \"I-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('England', 'B-LOC'), ('Pakistan', 'B-LOC'), ('Old', 'B-LOC'), ('Trafford', 'I-LOC')]\n",
            "\n",
            "Ture vector [[('England', 'B-LOC'), ('Pakistan', 'B-LOC'), ('Old', 'B-LOC'), ('Trafford', 'I-LOC')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 12. Johnny Herbert ( Britain ) Sauber 1:53.993\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Johnny\", \"B-PER\"), (\"Herbert\", \"I-PER\"), (\"Britain\", \"B-LOC\"), (\"Sauber\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Johnny', 'B-PER'), ('Herbert', 'I-PER'), ('Britain', 'B-LOC'), ('Sauber', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Johnny', 'B-PER'), ('Herbert', 'I-PER'), ('Britain', 'B-LOC'), ('Sauber', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Mauritius now play in group seven of the qualifiers against Malawi , Mozambique and favourites Zambia .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Mauritius\", \"B-LOC\"), (\"Malawi\", \"B-LOC\"), (\"Mozambique\", \"B-LOC\"), (\"Zambia\", \"B-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Mauritius', 'B-LOC'), ('Malawi', 'B-LOC'), ('Mozambique', 'B-LOC'), ('Zambia', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Mauritius', 'B-LOC'), ('Malawi', 'B-LOC'), ('Mozambique', 'B-LOC'), ('Zambia', 'B-LOC')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 1860 Munich 1 ( Schwabl 38th ) Borussia Dortmund 3 ( Zorc\n",
            "\n",
            "  Result:\n",
            "  \n",
            "An error occurred for sentence 20: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Hapoel Tel Aviv 1 Maccabi Haifa 3\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Hapoel\", \"B-ORG\"), (\"Tel\", \"I-ORG\"), (\"Aviv\", \"I-ORG\"), (\"Maccabi\", \"B-ORG\"), (\"Haifa\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Hapoel', 'B-ORG'), ('Tel', 'I-ORG'), ('Aviv', 'I-ORG'), ('Maccabi', 'B-ORG'), ('Haifa', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Hapoel', 'B-ORG'), ('Tel', 'I-ORG'), ('Aviv', 'I-ORG'), ('Maccabi', 'B-ORG'), ('Haifa', 'I-ORG')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: At least eight U.S. states have still some form of litigation pending , said John Head , spokesman for the Association of Lloyd 's State Chairmen , a group representing U.S. Names .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "An error occurred for sentence 22: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Mark Philippoussis beat fellow Australian Mark Woodforde 6-7 ( 6-8 ) 6-3 6-3 6-3 in a men 's singles first round match at the U.S. Open on Tuesday .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Mark\", \"B-PER\"), (\"Philippoussis\", \"I-PER\"), (\"Australian\", \"B-MISC\"), (\"Mark\", \"B-PER\"), (\"Woodforde\", \"I-PER\"), (\"U.S. Open\", \"B-MISC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Mark', 'B-PER'), ('Philippoussis', 'I-PER'), ('Australian', 'B-MISC'), ('Mark', 'B-PER'), ('Woodforde', 'I-PER'), ('U.S.', 'B-MISC'), ('Open', 'I-MISC')]\n",
            "\n",
            "Ture vector [[('Mark', 'B-PER'), ('Philippoussis', 'I-PER'), ('Australian', 'B-MISC'), ('Mark', 'B-PER'), ('Woodforde', 'I-PER'), ('U.S.', 'B-MISC'), ('Open', 'I-MISC')]]\n",
            "tp:  7\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Doug Flach ( U.S. ) beat Gianluca Pozzi ( Italy ) 7-5 7-6 ( 7-5 ) 2-6 7-6 ( 8-6 )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Doug\", \"B-PER\"), (\"Flach\", \"I-PER\"), (\"U.S.\", \"B-LOC\"), (\"Gianluca\", \"B-PER\"), (\"Pozzi\", \"I-PER\"), (\"Italy\", \"B-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Doug', 'B-PER'), ('Flach', 'I-PER'), ('U.S.', 'B-LOC'), ('Gianluca', 'B-PER'), ('Pozzi', 'I-PER'), ('Italy', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Doug', 'B-PER'), ('Flach', 'I-PER'), ('U.S.', 'B-LOC'), ('Gianluca', 'B-PER'), ('Pozzi', 'I-PER'), ('Italy', 'B-LOC')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 14. Olivier Panis ( France ) Ligier 1:54.220\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Olivier\", \"B-PER\"), (\"Panis\", \"I-PER\"), (\"France\", \"B-LOC\"), (\"Ligier\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Olivier', 'B-PER'), ('Panis', 'I-PER'), ('France', 'B-LOC'), ('Ligier', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Olivier', 'B-PER'), ('Panis', 'I-PER'), ('France', 'B-LOC'), ('Ligier', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Formula Shell beat Ginebra San Miguel 89-86 ( 45-46 half-time )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Formula\", \"B-ORG\"), (\"Shell\", \"I-ORG\"), (\"Ginebra\", \"B-ORG\"), (\"San\", \"I-ORG\"), (\"Miguel\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Formula', 'B-ORG'), ('Shell', 'I-ORG'), ('Ginebra', 'B-ORG'), ('San', 'I-ORG'), ('Miguel', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Formula', 'B-ORG'), ('Shell', 'I-ORG'), ('Ginebra', 'B-ORG'), ('San', 'I-ORG'), ('Miguel', 'I-ORG')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: ( N. Speak 65 not out , N. Fairbrother 55 ) .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"N.\", \"B-PER\"), (\"Speak\", \"I-PER\"), (\"N.\", \"B-PER\"), (\"Fairbrother\", \"I-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('N.', 'B-PER'), ('Speak', 'I-PER'), ('N.', 'B-PER'), ('Fairbrother', 'I-PER')]\n",
            "\n",
            "Ture vector [[('N.', 'B-PER'), ('Speak', 'I-PER'), ('N.', 'B-PER'), ('Fairbrother', 'I-PER')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Swede Kenneth Eriksson kept Subaru in the hunt for the manufacturers ' title with fifth place in spite of a gearbox problem that nearly forced him off the road close to the end of the event .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Swede\", \"B-MISC\"), (\"Kenneth\", \"B-PER\"), (\"Eriksson\", \"I-PER\"), (\"Subaru\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Swede', 'B-MISC'), ('Kenneth', 'B-PER'), ('Eriksson', 'I-PER'), ('Subaru', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Swede', 'B-MISC'), ('Kenneth', 'B-PER'), ('Eriksson', 'I-PER'), ('Subaru', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Other larger activist groups include Earth First , The Land is Ours , Alarm UK and Road Alert .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "An error occurred for sentence 29: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of the response using the true label\n",
        "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(\"Results: \")\n",
        "print(\"Recall: \", recall)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"F1: \", f1)"
      ],
      "metadata": {
        "id": "z1zC0C4i90lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c60bfdb-0f10-4935-a9f2-6dd071b2a910"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: \n",
            "Recall:  0.8951048951048951\n",
            "Precision:  0.9411764705882353\n",
            "F1:  0.9175627240143368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto write prompt for NER task\n",
        "\n",
        "autowrite = \"\"\"\n",
        "If you have to do the NER (Name Entity Recognition) task of a given phrase (I give you the phrase) in zero-shot environment (you don't see any\n",
        "data except the input phrase) and you should describe the best possible metodology that can help you to do such task in order to get the highest\n",
        "possible accuracy, how do you describe that?\n",
        "\n",
        "Consider that the dataset is already choosen and you should do only the NER task of a phrase, nothing else, but you have to achive the highest\n",
        "accuracy possible.\n",
        "\n",
        "Write down only a bullet point or a numbered list of tasks that can help you to perform the NER task . This list will be the prompt for the NER task, so no other\n",
        "generated content is allowed because the prompt must be as much clear as possible.\n",
        "\n",
        "Remember: you should describe a metodology that can help you (gemini 2.0-flash) to perform the task.\n",
        "\n",
        "Metodology:\n",
        "\"\"\"\n",
        "\n",
        "autowrite_response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=autowrite,\n",
        ")\n",
        "\n",
        "print(autowrite_response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgRvffNZ3oy-",
        "outputId": "35434e8d-699c-49d8-c6fb-2851dcc2ea82"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try the auto write prompt for NER task\n",
        "\n",
        "tp, fp, fn = 0, 0, 0\n",
        "\n",
        "# for i in range (0, len(dataset_sentences_array)):\n",
        "for i in range (0, 30):\n",
        "\n",
        "  # Prompt for the NER task\n",
        "  prompt = \"\"\"Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
        "  The only entity labels that could appear in the phrase are:\n",
        "    - Location (LOC).\n",
        "    - Organization (ORG).\n",
        "    - Person (PER).\n",
        "    - Miscellaneos (MISC).\n",
        "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
        "\n",
        "  \"\"\" + autowrite_response.text + \"\"\"\n",
        "\n",
        "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
        "\n",
        "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
        "\n",
        "  Answer with less words as possible.\n",
        "\n",
        "  Your phrase is: \"\"\" + dataset_sentences_array[i] + \"\"\"\n",
        "\n",
        "  Result:\n",
        "  \"\"\"\n",
        "\n",
        "  print(prompt)\n",
        "\n",
        "  try:\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        contents=prompt,\n",
        "    )\n",
        "\n",
        "    print(response.text)\n",
        "\n",
        "\n",
        "    tem_tp, tem_fp, tem_fn = metrics(response.text, wordpair_sentence_label[i])\n",
        "\n",
        "    print(\"\\nTure vector [\" + str(wordpair_sentence_label[i]) + \"]\")\n",
        "    print(\"tp: \", tem_tp)\n",
        "    print(\"fp: \", tem_fp)\n",
        "    print(\"fn: \", tem_fn)\n",
        "\n",
        "    tp += tem_tp\n",
        "    fp += tem_fp\n",
        "    fn += tem_fn\n",
        "\n",
        "    # Sleep to not exeed the API limit\n",
        "    time.sleep(6)\n",
        "\n",
        "  # An error occured (probably Gemini is overloaded), so wait 15 seconds\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred for sentence {i}: {e}\")\n",
        "    time.sleep(15)\n",
        "\n",
        "    # Try again\n",
        "    i = i - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L53gw-N98BwE",
        "outputId": "8100eec6-0e12-4da9-c8eb-0e3d39bce994"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Scorers : Ewald Brenner ( 5th minute ) , Mario Stieglmair ( 42nd ) , Ronald Brunmayr ( 43rd and 56th ) .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Ewald\", \"B-PER\"), (\"Brenner\", \"I-PER\"), (\"Mario\", \"B-PER\"), (\"Stieglmair\", \"I-PER\"), (\"Ronald\", \"B-PER\"), (\"Brunmayr\", \"I-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Ewald', 'B-PER'), ('Brenner', 'I-PER'), ('Mario', 'B-PER'), ('Stieglmair', 'I-PER'), ('Ronald', 'B-PER'), ('Brunmayr', 'I-PER')]\n",
            "\n",
            "Ture vector [[('Ewald', 'B-PER'), ('Brenner', 'I-PER'), ('Mario', 'B-PER'), ('Stieglmair', 'I-PER'), ('Ronald', 'B-PER'), ('Brunmayr', 'I-PER')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 144 Peter O'Malley ( Australia ) 71 73 , Costantino Rocca ( Italy )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"Peter\", \"B-PER\"), (\"O'Malley\", \"I-PER\"), (\"Australia\", \"B-LOC\"), (\"Costantino\", \"B-PER\"), (\"Rocca\", \"I-PER\"), (\"Italy\", \"B-LOC\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Peter', 'B-PER'), (\"O'Malley\", 'I-PER'), ('Australia', 'B-LOC'), ('Costantino', 'B-PER'), ('Rocca', 'I-PER'), ('Italy', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Peter', 'B-PER'), (\"O'Malley\", 'I-PER'), ('Australia', 'B-LOC'), ('Costantino', 'B-PER'), ('Rocca', 'I-PER'), ('Italy', 'B-LOC')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Leading rider Jason Weaver received a 21-day ban from the disciplinary committee of the Jockey Club on Wednesday .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Jason\", \"B-PER\"), (\"Weaver\", \"I-PER\"), (\"Jockey\", \"B-ORG\"), (\"Club\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Jason', 'B-PER'), ('Weaver', 'I-PER'), ('Jockey', 'B-ORG'), ('Club', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Jason', 'B-PER'), ('Weaver', 'I-PER'), ('Jockey', 'B-ORG'), ('Club', 'I-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: ( N. Fairbrother 86 , M. Watkinson 64 ; D. Gough 4-53 ) and 210-5\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"N.\", \"B-PER\"), (\"Fairbrother\", \"I-PER\"), (\"M.\", \"B-PER\"), (\"Watkinson\", \"I-PER\"), (\"D.\", \"B-PER\"), (\"Gough\", \"I-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('N.', 'B-PER'), ('Fairbrother', 'I-PER'), ('M.', 'B-PER'), ('Watkinson', 'I-PER'), ('D.', 'B-PER'), ('Gough', 'I-PER')]\n",
            "\n",
            "Ture vector [[('N.', 'B-PER'), ('Fairbrother', 'I-PER'), ('M.', 'B-PER'), ('Watkinson', 'I-PER'), ('D.', 'B-PER'), ('Gough', 'I-PER')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: COMEX copper ended higher after a late , modest recovery dragged the market from the lows , but traders shrugged-off an imminent strike at Codelco 's Salvador mine in Chile .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"COMEX\", \"B-ORG\"), (\"Codelco\", \"B-ORG\"), (\"Salvador\", \"B-LOC\"), (\"Chile\", \"B-LOC\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('COMEX', 'B-ORG'), ('Codelco', 'B-ORG'), ('Salvador', 'B-LOC'), ('Chile', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('COMEX', 'B-ORG'), ('Codelco', 'B-ORG'), ('Salvador', 'B-LOC'), ('Chile', 'B-LOC')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 220 Barry Lane 73 77 70 , Wayne Riley ( Australia ) 71 78 71 ,\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Barry\", \"B-PER\"), (\"Lane\", \"I-PER\"), (\"Wayne\", \"B-PER\"), (\"Riley\", \"I-PER\"), (\"Australia\", \"B-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Barry', 'B-PER'), ('Lane', 'I-PER'), ('Wayne', 'B-PER'), ('Riley', 'I-PER'), ('Australia', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Barry', 'B-PER'), ('Lane', 'I-PER'), ('Wayne', 'B-PER'), ('Riley', 'I-PER'), ('Australia', 'B-LOC')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: RUGBY LEAGUE - EUROPEAN SUPER LEAGUE RESULTS / STANDINGS .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"RUGBY\", \"B-MISC\"), (\"LEAGUE\", \"I-MISC\"), (\"EUROPEAN\", \"B-MISC\"), (\"SUPER\", \"I-MISC\"), (\"LEAGUE\", \"I-MISC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('RUGBY', 'B-MISC'), ('LEAGUE', 'I-MISC'), ('EUROPEAN', 'B-MISC'), ('SUPER', 'I-MISC'), ('LEAGUE', 'I-MISC')]\n",
            "\n",
            "Ture vector [[('RUGBY', 'B-MISC'), ('LEAGUE', 'I-MISC'), ('EUROPEAN', 'B-MISC'), ('SUPER', 'I-MISC'), ('LEAGUE', 'I-MISC')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 1. Takuma Aoki ( Japan ) Honda 38:18.759\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Takuma\", \"B-PER\"), (\"Aoki\", \"I-PER\"), (\"Japan\", \"B-LOC\"), (\"Honda\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Takuma', 'B-PER'), ('Aoki', 'I-PER'), ('Japan', 'B-LOC'), ('Honda', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Takuma', 'B-PER'), ('Aoki', 'I-PER'), ('Japan', 'B-LOC'), ('Honda', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Gloria Bistrita - Ilie Lazar ( 32nd ) , Eugen Voica ( 84th )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Gloria\", \"B-ORG\"), (\"Bistrita\", \"I-ORG\"), (\"Ilie\", \"B-PER\"), (\"Lazar\", \"I-PER\"), (\"Eugen\", \"B-PER\"), (\"Voica\", \"I-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Gloria', 'B-ORG'), ('Bistrita', 'I-ORG'), ('Ilie', 'B-PER'), ('Lazar', 'I-PER'), ('Eugen', 'B-PER'), ('Voica', 'I-PER')]\n",
            "\n",
            "Ture vector [[('Gloria', 'B-ORG'), ('Bistrita', 'I-ORG'), ('Ilie', 'B-PER'), ('Lazar', 'I-PER'), ('Eugen', 'B-PER'), ('Voica', 'I-PER')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: The ECOMOG force , currently 10,000 strong , was sent to Liberia by the Economic Community of West African States in 1990 at the height of the fighting .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```\n",
            "[(\"ECOMOG\", \"B-ORG\"), (\"Liberia\", \"B-LOC\"), (\"Economic Community of West African States\", \"B-ORG\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('ECOMOG', 'B-ORG'), ('Liberia', 'B-LOC'), ('Economic', 'B-ORG'), ('Community', 'I-ORG'), ('of', 'I-ORG'), ('West', 'I-ORG'), ('African', 'I-ORG'), ('States', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('ECOMOG', 'B-ORG'), ('Liberia', 'B-LOC'), ('Economic', 'B-ORG'), ('Community', 'I-ORG'), ('of', 'I-ORG'), ('West', 'I-ORG'), ('African', 'I-ORG'), ('States', 'I-ORG')]]\n",
            "tp:  8\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Phan 's accidental journey started last week in Prince Rupert , British Columbia , where he was searching for a fishing job , Jewell said .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"Phan\", \"B-PER\"), (\"Prince Rupert\", \"B-LOC\"), (\"British Columbia\", \"B-LOC\"), (\"Jewell\", \"B-PER\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Phan', 'B-PER'), ('British Columbia', 'B-LOC'), ('Jewell', 'B-PER'), ('Prince', 'B-LOC'), ('Rupert', 'I-LOC')]\n",
            "\n",
            "Ture vector [[('Phan', 'B-PER'), ('Prince', 'B-LOC'), ('Rupert', 'I-LOC'), ('British', 'B-LOC'), ('Columbia', 'I-LOC'), ('Jewell', 'B-PER')]]\n",
            "tp:  4\n",
            "fp:  1\n",
            "fn:  2\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: An earthquake measuring 5.5 on the Richter scale shook New Zealands upper South Island on Thursday but there were no reports of injuries , Television New Zealand said .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"New\", \"B-LOC\"), (\"Zealand\", \"I-LOC\"), (\"South\", \"B-LOC\"), (\"Island\", \"I-LOC\"), (\"Television\", \"B-ORG\"), (\"New\", \"I-ORG\"), (\"Zealand\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('New', 'B-LOC'), ('Zealand', 'I-LOC'), ('South', 'B-LOC'), ('Island', 'I-LOC'), ('Television', 'B-ORG'), ('New', 'I-ORG'), ('Zealand', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Richter', 'B-PER'), ('New', 'B-MISC'), ('Zealands', 'I-MISC'), ('South', 'B-LOC'), ('Island', 'I-LOC'), ('Television', 'B-ORG'), ('New', 'I-ORG'), ('Zealand', 'I-ORG')]]\n",
            "tp:  5\n",
            "fp:  2\n",
            "fn:  3\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Daewoo Dacom Communications ( Poland ) Ltd , which was set up with an initial investment of $ 1.0 million , is expected to have sales of $ 60 million by the year 2000 , a Daewoo statement said on Thursday ..\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"Daewoo\", \"B-ORG\"), (\"Dacom\", \"I-ORG\"), (\"Communications\", \"I-ORG\"), (\"Poland\", \"B-LOC\"), (\"Daewoo\", \"B-ORG\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Daewoo', 'B-ORG'), ('Dacom', 'I-ORG'), ('Communications', 'I-ORG'), ('Poland', 'B-LOC'), ('Daewoo', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Daewoo', 'B-ORG'), ('Dacom', 'I-ORG'), ('Communications', 'I-ORG'), ('Poland', 'B-LOC'), ('Ltd', 'B-ORG'), ('Daewoo', 'B-ORG')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  1\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Christie will run the anchor leg after Canada 's Bailey , American Johnson and Olympic silver medallist Fredericks have run the first three stages of the relay .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"Christie\", \"B-PER\"), (\"Canada\", \"B-LOC\"), (\"Bailey\", \"B-PER\"), (\"American\", \"B-MISC\"), (\"Johnson\", \"B-PER\"), (\"Fredericks\", \"B-PER\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Christie', 'B-PER'), ('Canada', 'B-LOC'), ('Bailey', 'B-PER'), ('American', 'B-MISC'), ('Johnson', 'B-PER'), ('Fredericks', 'B-PER')]\n",
            "\n",
            "Ture vector [[('Christie', 'B-PER'), ('Canada', 'B-LOC'), ('Bailey', 'B-PER'), ('American', 'B-MISC'), ('Johnson', 'B-PER'), ('Olympic', 'B-MISC'), ('Fredericks', 'B-PER')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  1\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Groningen took the lead in the seventh minute when Dean Gorre intercepted a back pass from Ernest Faber to goalkeeper Ronald Waterreus and shot home .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```\n",
            "[(\"Groningen\", \"B-LOC\"), (\"Dean Gorre\", \"B-PER\"), (\"Ernest Faber\", \"B-PER\"), (\"Ronald Waterreus\", \"B-PER\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Groningen', 'B-LOC'), ('Ernest Faber', 'B-PER'), ('Dean', 'B-PER'), ('Gorre', 'I-PER'), ('Ronald', 'B-PER'), ('Waterreus', 'I-PER')]\n",
            "\n",
            "Ture vector [[('Groningen', 'B-ORG'), ('Dean', 'B-PER'), ('Gorre', 'I-PER'), ('Ernest', 'B-PER'), ('Faber', 'I-PER'), ('Ronald', 'B-PER'), ('Waterreus', 'I-PER')]]\n",
            "tp:  4\n",
            "fp:  2\n",
            "fn:  3\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: The church in Australia said on Monday Lynch , Batchelor , Barton and Riel were held in a prison until the weekend , when they were moved to join the other captives at the compound .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```\n",
            "[(\"Australia\", \"B-LOC\"), (\"Lynch\", \"B-PER\"), (\"Batchelor\", \"B-PER\"), (\"Barton\", \"B-PER\"), (\"Riel\", \"B-PER\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Australia', 'B-LOC'), ('Lynch', 'B-PER'), ('Batchelor', 'B-PER'), ('Barton', 'B-PER'), ('Riel', 'B-PER')]\n",
            "\n",
            "Ture vector [[('Australia', 'B-LOC'), ('Lynch', 'B-PER'), ('Batchelor', 'B-PER'), ('Barton', 'B-PER'), ('Riel', 'B-LOC')]]\n",
            "tp:  4\n",
            "fp:  1\n",
            "fn:  1\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Manchester United 2 ( Cruyff 39th minute , Solskjaer 70th )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Manchester\", \"B-ORG\"), (\"United\", \"I-ORG\"), (\"Cruyff\", \"B-PER\"), (\"Solskjaer\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Manchester', 'B-ORG'), ('United', 'I-ORG'), ('Cruyff', 'B-PER'), ('Solskjaer', 'B-PER')]\n",
            "\n",
            "Ture vector [[('Manchester', 'B-ORG'), ('United', 'I-ORG'), ('Cruyff', 'B-PER'), ('Solskjaer', 'B-PER')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: England beat Pakistan by five wickets to win the first one-day ( 50 overs-a-side ) international at Old Trafford on Thursday .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"England\", \"B-LOC\"), (\"Pakistan\", \"B-LOC\"), (\"Old\", \"B-LOC\"), (\"Trafford\", \"I-LOC\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('England', 'B-LOC'), ('Pakistan', 'B-LOC'), ('Old', 'B-LOC'), ('Trafford', 'I-LOC')]\n",
            "\n",
            "Ture vector [[('England', 'B-LOC'), ('Pakistan', 'B-LOC'), ('Old', 'B-LOC'), ('Trafford', 'I-LOC')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 12. Johnny Herbert ( Britain ) Sauber 1:53.993\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Johnny\", \"B-PER\"), (\"Herbert\", \"I-PER\"), (\"Britain\", \"B-LOC\"), (\"Sauber\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Johnny', 'B-PER'), ('Herbert', 'I-PER'), ('Britain', 'B-LOC'), ('Sauber', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Johnny', 'B-PER'), ('Herbert', 'I-PER'), ('Britain', 'B-LOC'), ('Sauber', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Mauritius now play in group seven of the qualifiers against Malawi , Mozambique and favourites Zambia .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```\n",
            "[(\"Mauritius\", \"B-LOC\"), (\"Malawi\", \"B-LOC\"), (\"Mozambique\", \"B-LOC\"), (\"Zambia\", \"B-LOC\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('Mauritius', 'B-LOC'), ('Malawi', 'B-LOC'), ('Mozambique', 'B-LOC'), ('Zambia', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Mauritius', 'B-LOC'), ('Malawi', 'B-LOC'), ('Mozambique', 'B-LOC'), ('Zambia', 'B-LOC')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 1860 Munich 1 ( Schwabl 38th ) Borussia Dortmund 3 ( Zorc\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"1860\", \"B-MISC\"), (\"Munich\", \"I-MISC\"), (\"Schwabl\", \"B-PER\"), (\"Borussia\", \"B-ORG\"), (\"Dortmund\", \"I-ORG\"), (\"Zorc\", \"B-PER\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('1860', 'B-MISC'), ('Munich', 'I-MISC'), ('Schwabl', 'B-PER'), ('Borussia', 'B-ORG'), ('Dortmund', 'I-ORG'), ('Zorc', 'B-PER')]\n",
            "\n",
            "Ture vector [[('1860', 'B-ORG'), ('Munich', 'I-ORG'), ('Schwabl', 'B-PER'), ('Borussia', 'B-ORG'), ('Dortmund', 'I-ORG'), ('Zorc', 'B-PER')]]\n",
            "tp:  4\n",
            "fp:  2\n",
            "fn:  2\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Hapoel Tel Aviv 1 Maccabi Haifa 3\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Hapoel\", \"B-ORG\"), (\"Tel\", \"I-ORG\"), (\"Aviv\", \"I-ORG\"), (\"Maccabi\", \"B-ORG\"), (\"Haifa\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Hapoel', 'B-ORG'), ('Tel', 'I-ORG'), ('Aviv', 'I-ORG'), ('Maccabi', 'B-ORG'), ('Haifa', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Hapoel', 'B-ORG'), ('Tel', 'I-ORG'), ('Aviv', 'I-ORG'), ('Maccabi', 'B-ORG'), ('Haifa', 'I-ORG')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: At least eight U.S. states have still some form of litigation pending , said John Head , spokesman for the Association of Lloyd 's State Chairmen , a group representing U.S. Names .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "```python\n",
            "[(\"U.S.\", \"B-LOC\"), (\"John\", \"B-PER\"), (\"Head\", \"I-PER\"), (\"Association\", \"B-ORG\"), (\"of\", \"I-ORG\"), (\"Lloyd\", \"I-ORG\"), (\"'s\", \"I-ORG\"), (\"State\", \"I-ORG\"), (\"Chairmen\", \"I-ORG\"), (\"U.S.\", \"B-LOC\"), (\"Names\", \"B-MISC\")]\n",
            "```\n",
            "\n",
            "Predicted vector after normalization:  [('U.S.', 'B-LOC'), ('John', 'B-PER'), ('Head', 'I-PER'), ('Association', 'B-ORG'), ('of', 'I-ORG'), ('Lloyd', 'I-ORG'), (\"'s\", 'I-ORG'), ('State', 'I-ORG'), ('Chairmen', 'I-ORG'), ('U.S.', 'B-LOC'), ('Names', 'B-MISC')]\n",
            "\n",
            "Ture vector [[('U.S.', 'B-LOC'), ('John', 'B-PER'), ('Head', 'I-PER'), ('Association', 'B-ORG'), ('of', 'I-ORG'), ('Lloyd', 'I-ORG'), (\"'s\", 'I-ORG'), ('U.S.', 'B-LOC'), ('Names', 'B-MISC')]]\n",
            "tp:  9\n",
            "fp:  2\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Mark Philippoussis beat fellow Australian Mark Woodforde 6-7 ( 6-8 ) 6-3 6-3 6-3 in a men 's singles first round match at the U.S. Open on Tuesday .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "An error occurred for sentence 23: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Doug Flach ( U.S. ) beat Gianluca Pozzi ( Italy ) 7-5 7-6 ( 7-5 ) 2-6 7-6 ( 8-6 )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Doug\", \"B-PER\"), (\"Flach\", \"I-PER\"), (\"U.S.\", \"B-LOC\"), (\"Gianluca\", \"B-PER\"), (\"Pozzi\", \"I-PER\"), (\"Italy\", \"B-LOC\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Doug', 'B-PER'), ('Flach', 'I-PER'), ('U.S.', 'B-LOC'), ('Gianluca', 'B-PER'), ('Pozzi', 'I-PER'), ('Italy', 'B-LOC')]\n",
            "\n",
            "Ture vector [[('Doug', 'B-PER'), ('Flach', 'I-PER'), ('U.S.', 'B-LOC'), ('Gianluca', 'B-PER'), ('Pozzi', 'I-PER'), ('Italy', 'B-LOC')]]\n",
            "tp:  6\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: 14. Olivier Panis ( France ) Ligier 1:54.220\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Olivier\", \"B-PER\"), (\"Panis\", \"I-PER\"), (\"France\", \"B-LOC\"), (\"Ligier\", \"B-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Olivier', 'B-PER'), ('Panis', 'I-PER'), ('France', 'B-LOC'), ('Ligier', 'B-ORG')]\n",
            "\n",
            "Ture vector [[('Olivier', 'B-PER'), ('Panis', 'I-PER'), ('France', 'B-LOC'), ('Ligier', 'B-ORG')]]\n",
            "tp:  4\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Formula Shell beat Ginebra San Miguel 89-86 ( 45-46 half-time )\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Formula\", \"B-ORG\"), (\"Shell\", \"I-ORG\"), (\"Ginebra\", \"B-ORG\"), (\"San\", \"I-ORG\"), (\"Miguel\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Formula', 'B-ORG'), ('Shell', 'I-ORG'), ('Ginebra', 'B-ORG'), ('San', 'I-ORG'), ('Miguel', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Formula', 'B-ORG'), ('Shell', 'I-ORG'), ('Ginebra', 'B-ORG'), ('San', 'I-ORG'), ('Miguel', 'I-ORG')]]\n",
            "tp:  5\n",
            "fp:  0\n",
            "fn:  0\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: ( N. Speak 65 not out , N. Fairbrother 55 ) .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"N.\", \"B-PER\"), (\"Fairbrother\", \"B-PER\")]\n",
            "\n",
            "Predicted vector after normalization:  [('N.', 'B-PER'), ('Fairbrother', 'B-PER')]\n",
            "\n",
            "Ture vector [[('N.', 'B-PER'), ('Speak', 'I-PER'), ('N.', 'B-PER'), ('Fairbrother', 'I-PER')]]\n",
            "tp:  1\n",
            "fp:  1\n",
            "fn:  2\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Swede Kenneth Eriksson kept Subaru in the hunt for the manufacturers ' title with fifth place in spite of a gearbox problem that nearly forced him off the road close to the end of the event .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "An error occurred for sentence 28: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}\n",
            "Do the Name Entity Recognition (also known as NER) task to a following phrase I will give you.\n",
            "  The only entity labels that could appear in the phrase are:\n",
            "    - Location (LOC).\n",
            "    - Organization (ORG).\n",
            "    - Person (PER).\n",
            "    - Miscellaneos (MISC).\n",
            "  You have to use prefixes B (for begin) or I (for inside) together with a tag in the list ahead (for example use \"B-PER\" and not just \"PER\").\n",
            "\n",
            "  Here's the methodology as a bullet point list, designed to guide a zero-shot NER task for a single phrase with maximum accuracy:\n",
            "\n",
            "*   **Decompose the Phrase:** Break the input phrase into individual words or sub-phrases.\n",
            "\n",
            "*   **Entity Type Priming:** Before processing, explicitly define the entity types that are relevant to the target dataset. Example: \"Recognize PERSON, ORGANIZATION, LOCATION, DATE, TIME, and MISC entities.\"\n",
            "\n",
            "*   **Prompt Engineering with Examples (Illustrative):** Construct prompts that include *hypothetical* examples of NER applied to similar (but distinct) phrases. The examples would demonstrate the desired output format and NER style.\n",
            "\n",
            "*   **Leverage Contextual Clues:** Focus on identifying contextual cues within the phrase that indicate entity types (e.g., prepositions like \"in\" often precede LOCATION, titles like \"Dr.\" precede PERSON).\n",
            "\n",
            "*   **Iterative Refinement:** Start with a general prompt. Analyze the initial output and then refine the prompt to address any inaccuracies or omissions. The refined prompt is only a variation of the initial one, with more detail, for instance.\n",
            "\n",
            "*   **Confidence Scoring Analysis (if available):** If the model provides confidence scores for its predictions, analyze those scores to identify potentially uncertain entity assignments. Manually review entities with low confidence scores.\n",
            "\n",
            "*   **Synonym and Alias Expansion (For Potentially Ambiguous Words):** Before final submission, consider potential synonyms or aliases for identified entities. If an entity is ambiguous, check if any synonyms clarify the true entity type within the given context. (e.g., Is \"Apple\" the company or a fruit?).\n",
            "\n",
            "*   **Output Formatting:** Ensure the final output is in a consistent, easily parsable format, clearly indicating the identified entity and its type. Example: `[Entity: entity_value, Type: entity_type]`.\n",
            "\n",
            "\n",
            "  You must keep only the words with a relevant tag (the ones I've listed before).\n",
            "\n",
            "  The result must be in a list of tuples of word-tag like: (\"Albert\", \"B-PER\").\n",
            "\n",
            "  Answer with less words as possible.\n",
            "\n",
            "  Your phrase is: Other larger activist groups include Earth First , The Land is Ours , Alarm UK and Road Alert .\n",
            "\n",
            "  Result:\n",
            "  \n",
            "[(\"Earth\", \"B-ORG\"), (\"First\", \"I-ORG\"), (\",\", \"MISC\"), (\"The\", \"B-ORG\"), (\"Land\", \"I-ORG\"), (\"is\", \"I-ORG\"), (\"Ours\", \"I-ORG\"), (\",\", \"MISC\"), (\"Alarm\", \"B-ORG\"), (\"UK\", \"I-ORG\"), (\"and\", \"MISC\"), (\"Road\", \"B-ORG\"), (\"Alert\", \"I-ORG\")]\n",
            "\n",
            "Predicted vector after normalization:  [('Earth', 'B-ORG'), ('First', 'I-ORG'), (',', 'MISC'), ('The', 'B-ORG'), ('Land', 'I-ORG'), ('is', 'I-ORG'), ('Ours', 'I-ORG'), (',', 'MISC'), ('Alarm', 'B-ORG'), ('UK', 'I-ORG'), ('and', 'MISC'), ('Road', 'B-ORG'), ('Alert', 'I-ORG')]\n",
            "\n",
            "Ture vector [[('Earth', 'B-ORG'), ('First', 'I-ORG'), ('The', 'B-ORG'), ('Land', 'I-ORG'), ('is', 'I-ORG'), ('Ours', 'I-ORG'), ('Alarm', 'B-ORG'), ('UK', 'I-ORG'), ('Road', 'B-ORG'), ('Alert', 'I-ORG')]]\n",
            "tp:  10\n",
            "fp:  3\n",
            "fn:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of the response using the true label\n",
        "precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "print(\"Results: \")\n",
        "print(\"Recall: \", recall)\n",
        "print(\"Precision: \", precision)\n",
        "print(\"F1: \", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndhfo7Kl4dLo",
        "outputId": "378a7a21-1e2f-4827-ee40-b2cef5b2590f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results: \n",
            "Recall:  0.9044585987261147\n",
            "Precision:  0.9102564102564102\n",
            "F1:  0.9073482428115015\n"
          ]
        }
      ]
    }
  ]
}